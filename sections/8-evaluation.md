\clearpage

# Evaluation

The overarching goal of this project is to enable the benefits of mutation testing to be realised for real codebases, through the medium of CI, so it seems appropriate to evaluate it with respect to some existing open-source projects. The three such projects used throughout this chapter are detailed in the table below.

---------------------------------------------------------------------------------------
Project    GitHub repository          Description (from GitHub)
---------- -------------------------- -------------------------------------------------
event\_bus kevinrutherford/event\_bus A simple pubsub event bus.

full\_name agilidee/full\_name        Adds `full_name` method for classes that provide
                                      a `first_name` and a `last_name`!

lumberjack bdurand/lumberjack         A simple, powerful, and very fast logging utility
                                      that can be a drop in replacement for `Logger` or
                                      `ActiveSupport::BufferedLogger`.
---------------------------------------------------------------------------------------

These projects were chosen from a list of projects amenable to mutation testing provided in Mutant's documentation[@mlist], and from those projects' dependencies. Many codebases were eliminated from consideration because they were unsupported by Mutiny, which was at version 0.3.0 at the time. Mutiny's lack of compatibility does limit somewhat the applicability of this project but it is important to remember that MultiMutiny is, to a certain extent, independent of the underlying tool and that, as bugs in Mutiny are found and fixed, the pool of potential MultiMutiny targets will expand automatically.

## Functionality

This section will be quite brief because, in short, MultiMutiny works as expected. Applying it to the three projects under consideration yielded the results summarised in the table below. Complete copies of the command-line report arising from each project are given in Appendix 5, with a build without CI features, a passing CI build and a failing CI build each being demonstrated. Links to uploaded versions of HTML reports and to live reports on the cloud system are given in Appendix 6.

Project    Mutants generated Mutants killed Pessimistic mutation score
---------- ----------------- -------------- --------------------------
event\_bus 57                51             89%
full\_name 51                51             100%
lumberjack 565               480            85%

It is also quite clear that the software provides sufficient affordances as to allow it to operate as desired when called from CI programs. What should be highlighted in particular, given that it isn't evidenced in the reports in the two appendices, is that the correct exit codes are set to signal whether a given CI build has passed or failed.

As the results from the lumberjack project in particular demonstrate, MultiMutiny can cope with reasonable-sized projects and with a relatively large number of mutants. In fact, I can think of no hard limit as to the number of mutants that can be accommodated by the CLI and local HTML reporters.

For the cloud reporter, there are two limits set by the underlying Google App Engine platform that may prove to be problematic as the mutant count increases: The first of these is a 32MB limit imposed in each incoming HTTP request[@gaequotas], including the PUT requests used to transmit reports to the server. This shouldn't prove a huge problem given that it is quite a high limit and that compression could quite easily be added to requests: Given that the JSON reports contain a large amount of repeated data, with each mutant's entry containing a copy of the original and mutated files for instance, a significant reduction in file size should be easily achievable. The JSON file generated by testing the lumberjack project is 5.7MB (so still comfortably below the limit) uncompressed, and applying zlib compression reduces it to only 67KB.

A 1MB limit on datastore entries[@gaedatastore] may prove to be more of a problem, as each report is presently stored as a single entry. The lumberjack report would exceed this limit, were compression not applied. It would not be massively complicated however, were the need to arise, to split reports up into multiple entities, perhaps one per mutant scored, and thereby remove this worry entirely.

One piece of functionality that MultiMutiny's cloud component is lacking, when compared to commercial code coverage tools such as Coveralls[@coveralls] and Codecov[@codecov], is the provision of aggregate statistics for a project. Figures 10 and 11 give different visualisations, from these two products, of the progression in scores for example repositories. It would not be a huge technical undertaking to add something similar to MultiMutiny though: As can be inferred from Appendix 3, all the necessary information is already stored in a machine-queryable format, and the existing API keys could be repurposed to serve as a target project's unique identifier. The more challenging task would be to determine which statistics are actually valuable to developers, and the format in which to present them.

## Speed

One of the main aims of this project was to make a contribution to speeding-up the mutation testing process, so as to make it more practical for CI use. For the reasons given in the Analysis chapter, I chose to do this by parallelising the mutant scoring process. The table below gives the average times taken in testing each of the three projects mentioned above, as performed by Mutiny and by MultiMutiny (in benchmark mode) using four parallel processes.

Project     Mutiny   MultiMutiny  MultiMutiny as a % of Mutiny
---------- ------- ------------- -----------------------------
event\_bus  17.32s         7.20s                        41.59%
full\_name  15.30s         6.55s                        42.79%
lumberjack 142.68s        51.78s                        36.29%

All timings in the above table were recorded on the same quad-core machine as was used to create the benchmarks in the Analysis chapter. MultiMutiny was run in benchmark mode and four scoring processes were specified. Times given are in seconds, are rounded to two decimal places and are mean averages from five runs against each project.

As can be seen from these results, the speed increase afforded by MultiMutiny's parallelisation system is both significant and broadly in-line with (or slightly better than) that expected given the results recorded in the Analysis chapter. To put these results into context, recalling the aforementioned "ten minute rule" for useful CI builds, this is the equivalent of bringing projects that would normally take up to around 27&frac12; minutes to mutation test within that limit.

That said, and as noted earlier, the algorithm used is rather naive and thus far from optimal. The next part of this section will attempt to quantify that sub-optimality somewhat, by means of the results (and in particular the timings for each mutant) returned for the run against the full\_name project shown in Appendix 5.

The table overleaf shows the actual allocation of mutants to scoring processes (and thus processor cores) produced by MultiMutiny in that run[^ordering], together with the time taken in scoring each mutant. That time includes only that spent on the body of the tests, not that setting up or loading them, which is why the totals (in bold) end up being much less than the actual runtime given previously. Those overheads should at least be more or less constant for each mutant however, so shouldn't affect the validity of the conclusions drawn.

\clearpage

1              2              3              4
-------------- -------------- -------------- --------------
0 (0.04193s)   1 (0.042936s)  4 (0.040465s)  3 (0.039877s)
2 (0.044003s)  5 (0.041161s)  9 (0.035455s)  6 (0.038068s)
7 (0.036082s)  8 (0.03985s)   10 (0.038636s) 11 (0.038325s)
12 (0.039776s) 13 (0.046286s) 14 (0.039437s) 15 (0.027894s)
16 (0.040467s) 17 (0.032095s) 18 (0.027813s) 19 (0.027666s)
23 (0.028366s) 20 (0.042332s) 21 (0.027737s) 22 (0.028771s)
27 (0.028093s) 24 (0.029405s) 25 (0.029342s) 26 (0.026229s)
30 (0.026223s) 28 (0.030884s) 29 (0.027865s) 33 (0.026222s)
34 (0.029065s) 31 (0.027676s) 32 (0.026059s) 37 (0.028659s)
38 (0.026948s) 35 (0.032234s) 36 (0.027367s) 40 (0.027033s)
41 (0.028182s) 39 (0.028032s) 43 (0.028302s) 44 (0.027506s)
45 (0.027445s) 42 (0.028905s) 47 (0.026493s) 48 (0.02702s)
49 (0.029148s) 46 (0.026962s) 50 (0.02864s)  &nbsp;
**0.425728s**  **0.448758s**  **0.403611s**  **0.36327s**

This gives a total execution time of 0.448758 seconds. It is important to note however that, were the mutants generated by Mutiny in a different order, the worst possible allocation that could be achieved (namely the 13 most time-consuming mutants assigned to the same processor) would lead to a time of 0.537156 seconds.

With a list of jobs (each mutant requiring scoring) and their execution times, finding an optimal allocation with which to compare the above allocations is a classic example of the multiprocessor scheduling problem. A complete description of that problem would be beyond the scope of this section but it suffices to state that it is a NP-complete problem and that the longest processing time-first (or LPT) algorithm provides a reasonable approximation[@multiprocessor]. LPT scheduling requires sorting the jobs in non-increasing execution time order and assigning each job, in turn, to the processor with the earliest expected finish time[@multiprocessor]. Applied to the set of mutants above, it gives the allocation below.

1              2               3              4
-------------- --------------- -------------- --------------
4 (0.040465s)   2 (0.044003s)  1 (0.042936s)  0 (0.04193s)
7 (0.036082s)   6 (0.038068s)  3 (0.039877s)  8 (0.03985s)
9 (0.035455s)   12 (0.039776s) 5 (0.041161s)  11 (0.038325s)
13 (0.046286s)  16 (0.040467s) 10 (0.038636s) 15 (0.027894s)
14 (0.039437s)  19 (0.027666s) 21 (0.027737s) 17 (0.032095s)
18 (0.027813s)  22 (0.028771s) 23 (0.028366s) 20 (0.042332s)
26 (0.026229s)  29 (0.027865s) 24 (0.029405s) 25 (0.029342s)
27 (0.028093s)  33 (0.026222s) 28 (0.030884s) 30 (0.026223s)
34 (0.029065s)  35 (0.032234s) 32 (0.026059s) 31 (0.027676s)
44 (0.027506s)  40 (0.027033s) 38 (0.026948s) 36 (0.027367s)
48 (0.02702s)   43 (0.028302s) 39 (0.028032s) 37 (0.028659s)
50 (0.02864s)   46 (0.026962s) 42 (0.028905s) 41 (0.028182s)
&nbsp;          49 (0.029148s) 45 (0.027445s) 47 (0.026493s)
**0.392091s**   **0.416517s**  **0.416391s**  **0.416368s**

This gives a total execution time of 0.416517 seconds, a modest saving in absolute terms when compared to the actual and worst-case allocations but a reasonably significant one when expressed in percentage form: A saving of just over 7% is achieved against the actual allocation, with a more dramatic saving of just over 23% against the worst-case scenario. Both of these percentages represent much smaller increases in performance than the one achieved by moving from Mutiny to MultiMutiny.

Of course, it would be unreasonable to expect MultiMutiny to be able to achieve such an allocation without advance knowledge of the mutant scoring times. However, as will be expanded upon in the Conclusion chapter, it may well be possible to utilise saved state from previous scoring runs to approximate the application of the approximation algorithm given above.

![An example coverage progression graph, as produced by Codecov.](images/codecov.png)

![Coverage changes for another example repository, as shown by Coveralls.](images/coveralls.png)

[^ordering]: The reason why the allocation may seem slightly odd in the light of the explanation given in the previous chapter is that, at the point where allocation takes place, the mutants are in strict alphabetical order, i.e. 1 is followed by 10 rather than 2.
