\clearpage

# Literature Review

The two main concepts that this project builds upon are mutation testing and continuous integration. Both of these are related to testing software, and thereby evaluating software quality, so this section begins with a brief overview of the basic principles of software testing, before moving on to automated software testing methods. Code coverage measurements are then explained, before mutation testing is introduced as a stronger measure of the quality of an automated test suite. A fairly brief overview of continuous integration, again in the context of automated testing, is then given before the narrative moves on to two of the main challenges associated with, to quote the project title, "including mutation testing as part of a continuous integration workflow", work in the literature related to addressing those challenges, and finally the goal and motivations of this project.

## Manual and automated testing

For as long as there has been software, there has been an obvious need to ensure that that software does what it is supposed to do. In other words, to ensure that the software is of high enough quality, and that it meets the relevant requirements. The wider discipline of software testing has been the subject of innumerable papers and textbooks and is nowadays quite broadly defined in the literature, encompassing both finding bugs and the, in some cases quite subjective, evaluation of everything from program architecture to user interface accessibility and the comprehensiveness of documentation[@complete 6]. Important as those wider measures of quality are, it is the traditional (and some say outdated[@complete 5]) definition of testing, as finding bugs, that is relevant for this project.

The apocryphal original computer bug was a moth found in one of the relays of a Harvard University computer in the mid-1940s, and subsequently sticky-taped into the pages of a logbook to be preserved for all time[@bug]. Today however, the bugs found in programs are more likely to be of a less literal nature: They are implementation problems (i.e. incorrect code) within the software itself, that cause an incorrect output or a failure to output anything at all. To elaborate on the above more formally, and using the definitions given in [@introduction 12], the defect within the source code is known as a *fault*, an erroneous internal state arising from that fault is an *error* (which may or may not be corrected through, for instance, exception handling code), while an external manifestation of an error (such as the infamous "blue screen of death") is a *failure*. In this language, *testing* is the process of finding failures, while *debugging* is that of finding a fault given a failure.

The original, and least complex, method of testing is manual testing carried out by human testers. Sample inputs are given to a system and the output compared, by the tester, to that which is expected. Imagine a program, known as `caseflip` and referenced throughout this section, that transforms a single input character according to a simple rule: If the input character is lowercase it will be transformed to uppercase and vice-versa, otherwise it will be output as-is. Thus, `a` will become `A`, `Z` will become `z` and `6` and `+` will both remain unchanged. The aforementioned examples would actually make reasonably good manual test cases, at least in the absence of any knowledge of the system's internal mechanisms. Testing without knowledge of a program's source code is known as *black-box* testing, while testing with that knowledge, which can help tailor tests to the actual code-paths within the program, is *white-box* testing[@handbook 124]. It is the latter type of testing with which the remainder of this report is concerned, as it only with that type that mutation testing (and computing other test quality metrics) is possible.

``` {.python}
def flip(char):
  codepoint = ord(char)

  if 64 < codepoint < 91:
    return chr(codepoint + 32)

  if 96 < codepoint < 123:
    return chr(codepoint - 32)

  return char
```
Code Listing 1: The important part of the source code for the `caseflip` application. A fuller code listing, which includes "boilerplate" code omitted here, can be found at https://github.com/timw6n/caseflip. The "magic numbers" in the `if` conditions represent the ASCII codepoints of various characters (see Appendix 1 for more details).

Straight away, some of the problems inherent in manual testing become clear: Relying on people to sit there performing tasks and checking output is expensive, in terms of time and thereby money. Books (such as [@complete]) written prior to the widespread adoption of automated testing frequently state that over half of the total cost in a software project is spent on testing[@complete 3]. Moreover, the fact that manual testing is time-consuming both limits the number of tests that can be run to validate a change and tends to mean that testing is confined to a set period immediately prior to a release, rather than a full set of tests being run after smaller milestones. Manual testing is also error-prone, in that human testers may well miss subtle failures encountered in the course of a long and repetitive process. Monitoring a stream of output for errors, a common task during testing, is not entirely dissimilar to, for instance, monitoring CCTV monitors for untoward movements, and, for the latter task, there is literature pointing to a "vigilance decrement" after 20-30 minutes of work, after which the detection of something amiss becomes less likely[@cctv].

Thankfully, quickly and repeatedly performing simple tasks is something computers are rather better at and, as one might expect, there have long existed a variety of tools to automate parts of the testing workflow. One of the most well-known unit testing frameworks (unit testing here being defined in the broadest possible sense, encompassing a lot of what might strictly be termed integration testing) is the open-source JUnit framework for Java. The code sample below shows unit tests for the example application introduced above, written using the `unittest` module in Python's standard library. This module was itself inspired by JUnit and has many similar features[@unittest]. In systems of this type, a *test case* (such as `testLowercaseA` below) is an individual method containing (if it is to be useful) one or more assertions, while a *test suite* is the full set of test cases relevant to a program[@handbook 124]. Test suites can be sub-divided into one or more *test classes*, which usually mirror a similarly-named class in the main program. `TestFlippingCharacters` in the example below is a test class. A framework also provides for setup and tear-down methods to ensure isolation between different test cases and classes, as well as reporting systems to catch errors and provide statistics[@junit 10].

``` {.python}
class TestFlippingCharacters(unittest.TestCase):
    def testLowercaseA(self):
        self.assertEqual(flip('a'), 'A')

    def testLowercaseZ(self):
        self.assertEqual(flip('z'), 'Z')

    def testNumberOne(self):
        self.assertEqual(flip('1'), '1')

    def testUppercaseZ(self):
        self.assertEqual(flip('Z'), 'z')

    def testUppercaseA(self):
        self.assertEqual(flip('A'), 'a')

    def testPlusSymbol(self):
        self.assertEqual(flip('+'), '+')
```
Code Listing 2: Example unit tests for the method defined in Listing 1. Again, the full source file can be found at https://github.com/timw6n/caseflip.

Over in the Ruby world, which will be more relevant for the main part of this report, the most frequently-used testing framework seems to be RSpec. A set of example RSpec tests are given as the second part of Appendix 2, and, as can be seen, while the structure and keywords used are somewhat different from JUnit-type systems, what is accomplished is more-or-less the same. The "spec" in the name is short for specification, reflecting the fact that the "specs", or tests, for a class or method are often written prior to the code they test and serve as an automatically-checkable description of the required functionality[@rspec 3-6]. This methodology is referred to as test-driven development, but, for the purposes of this report, the point when the tests were written, or the terminology used to describe them, matters little.

One final point to note regarding our example application is that this rather trivial program *can* be tested exhaustively. It would be straightforward to write a set of 128 tests (one for every ASCII character) and be confident that the resulting test suite verifies every possible program output. No further evaluation of the test suite's quality would be necessary. However, exhaustive testing is often impractical, if not impossible, for most real programs. The range of potential input combinations for a simulator for the board game Go, for instance, greatly exceeds the number of atoms in the universe[@go] while there are many programs, particularly those accepting arbitrary user inputs, for which the number of potential inputs are effectively infinite.

If one accepts that truly exhaustive testing is not possible for a program, then it follows that there is no set of tests that can guarantee to find every fault[@handbook 128]. That is not to say that there are no tools available to estimate the quality of a given test suite, and that such heuristic measures are not useful in practice, just that they cannot be taken as absolute. Two such test-quality scoring measures, namely code coverage and mutation testing scores will now be introduced.

## Code coverage

Code coverage metrics are one of the most widely used indicators of the comprehensiveness of the tests for a given codebase. To simplify slightly, a given chunk of code is covered if, at some point during a test run, it is executed. There are several different levels of code coverage defined in the literature, with the most widely supported in commercial tooling being *statement coverage*, or C~0~[@covsurvey]. As implied by its abbreviation, this is the weakest code coverage metric: Several other, stronger but consequently more difficult to implement and to satisfy, coverage criterions also exist[@handbook 124] but need not be defined in this report.

C~0~ determines whether each logical statement is executed[@handbook 137] and is sometimes, slightly misleadingly, referred to as *line coverage*[@icse] since the usual practice for most languages is to have one statement per line. Returning to the example `caseflip` application and tests, the final `return char` line is not covered using this criterion, as each character used in the tests will trigger one of the two `if` blocks. A test using a character with a codepoint of 123 or higher (such as `|`) would be needed for full coverage.

In a practical context, code coverage tools are usually used to produce a percentage score, which represents the proportion of statements covered. This can be used as a measure of a test suite's quality, since uncovered code is effectively untested. Some organisations also set a minimum score as a hurdle that must be passed before a version can be released, though, as will be expanded upon later, this practice can be problematic.

## Mutation testing

Mutation testing, one of the aforementioned two pillars upon which this project is built, was first proposed in 1971 and the first related paper was published six years later[@mtcarol]. Over the following four decades, the field has attracted continued and increasing academic interest: In [@survey], a survey of papers published between 1977 and 2009, 393 mutation testing-related papers were identified, 51 of which were published in 2009. One of the more prolific authors on the subject is Jeff Offutt, and indeed his papers form a large part of this project's bibliography.

One way to explain mutation testing is as a fault-injection technique for software[@mimd 1], the virtual equivalent of techniques intended to test the resilience of computer systems to hardware faults. Instead of injecting faults into a running system through man-in-the-middle devices on input and output pins, or through bombarding CPUs with heavy-ion radiation[@fi], mutation testing modifies the program-under-test's source code prior to runtime. This is done by applying a number of *mutation operators* to the program's code, each of which makes a single change, injecting a single fault and thereby creating a *mutant*[@mimd].

These mutation operators, conventionally referred to by three-letter abbreviations, range from the generally-applicable to the highly language-specific: The ROR, or Relational Operator Replacement, operator substitutes relational operators for either a closely-related operator (e.g. `<` to `<=` as shown in Listing 3 below) or for the opposite operator (e.g. `==` to `!=`)[@oppy] and is implemented across a large number of mutation testing tools, for a variety of languages[@oppy]. At the other end of the spectrum are highly language-dependent operators such as the SIR operator targeting Python programs, which removes one element from a collection slice operator (e.g. `collection[3:8:2]` to `collection[:8:2]`)[@oppy].

\clearpage

``` {.python}
def flip(char):
  codepoint = ord(char)

  if 64 < codepoint <= 91:
    return chr(codepoint + 32)

  if 96 < codepoint < 123:
    return chr(codepoint - 32)

  return char
```
Code Listing 3: One possible mutated version of Listing 1. Note the `<=` in line 4, column 27.

The next step in the mutation testing process is to *score* the mutants, that is to execute the application's tests against each mutant. In the context of this project, this means running the relevant automated test suite. Those mutants for which a test failed (i.e. those that the test suite could distinguish from the original program) are referred to as having been *killed*, while mutants for which all tests passed have *survived*[@mimd]. Sometimes there are also *invalid*, *incompetent* or *stillborn* mutants that won't compile (in the context of a static language such as Java) or fail immediately at runtime (in the case of dynamic languages such as Python or Ruby), usually because of a type violation[@oppy]. These are counted within the killed group, despite not imparting any information with regard to test suite quality.

The main metric gleaned from the mutation testing process is the *mutation score* or *mutation adequacy score*[@asmt]. This is usually defined as the number of killed mutants over the total number of mutants created[@asmt], and is generally expressed as a percentage. Another definition that is sometimes seen in the literature, particularly in Offutt's papers, including [@mt2000] and [@mimd], is of the number of killed mutants over the total number of **non-equivalent** mutants. An equivalent mutant is a mutant that functions identically in all respects to the original program[@mt2000] and so cannot be killed by even a perfect test suite. As a trivial example, the logical statements `a && b` and `!(!a || !b)` are equivalent. For reasons that will be explained later in this report, the latter definition is somewhat impractical in creating automated mutation testing tools.

![The Mariner I rocket taking off[@m1image]. Less than five minutes later the rocket had to be destroyed after a crash became inevitable[@nasam1]. The cause was a missing hyphen in the code for the guidance program[@nasam1] — just the sort of tiny error mutation testing techniques could potentially catch.](images/mariner-one.jpg)

Another way to think of mutation testing is as a stronger form of code coverage. Recall from above that a given line of code is covered merely by being executed by a test. The fact that a line has mutation coverage provides a stronger assurance; information as to whether the tests actually verify the contents of that line.

A more formal argument sometimes seen is that mutation testing *subsumes* code coverage testing[@introduction 174]. An intuitive proof for this is quite convincing: Assuming that the mutation testing tool is sufficiently capable so as not to produce invalid or incompetent mutants, for a mutation on a line to affect the test outcome, that line must have been executed, otherwise its contents would be irrelevant. However, the devil is in the detail, in that this subsumption relationship is dependent on precisely which mutation operators are used — if a mutation tool has no operators capable of mutating the statements on a given line then it cannot impart any information with regard to that line.

\clearpage

A more convincing argument favouring mutation testing over code coverage comes from empirical research in [@effectiveness] and [@appropriate]. These two papers found evidence that, respectively, a high code coverage score is not strongly correlated with test suite effectiveness, whereas a high mutation score is.

The above point regarding the assurances that mutation coverage can provide gives us the main reason why mutation testing is useful in real-world scenarios. If the program's tests cannot differentiate between the original code and the mutated code, then there is no assurance as to which version encapsulates the correct logic: A surviving mutant could represent the correct version of the program. Additional justification for this comes from two widely accepted software testing hypotheses; the competent programmer hypothesis and the coupling hypothesis. The former states that a hypothetical competent programmer will tend to write code that reasonably closely resembles an equally hypothetical "correct" version of the program in question[@mimd]. In other words, the actual and ideal versions of a program will differ only by a few faults. Thus, the faults injected by the mutation testing process will be similar to those inadvertently added by real coders.

The coupling hypothesis states that a test suite sufficiently comprehensive as to detect all simple faults in a program will thereby also detect more complex faults[@mimd]. More succinctly, complex faults are *coupled* to simple faults. In the context of mutation testing, this provides another point of justification for operators only making a single, simple change. It also explains why higher-order mutants, those created by mutating mutants[@anpy], are rarely used and mostly considered unnecessary.

## Continuous integration

The second main theme for this project is continuous integration or CI. This section will be somewhat shorter than that relating to mutation testing, both because CI systems are simpler and easier to understand and because, while important for commercial software companies, the topic has attracted less academic interest. A literature review conducted by the authors of [@pracdiff] in 2013 discovered only 76 CI-related papers, far fewer than were found in the similar exercise for mutation testing mentioned above.

To backtrack a little, understanding the environment in which CI practices first arose requires one to imagine a stereotypical "waterfall" development process, in which programming stops and the software is deemed feature-complete before it is first built and all the components fitted together, or integrated. Only then does testing commence[@agile]. The practice of continuous integration, on the other hand, requires the integration and testing process be carried out continuously (hence the name) alongside development[@agile]. The need to run builds and tests so frequently implicitly requires that the process be automated, as does the original definition of continuous integration as one of the principles of the "extreme programming" methodology[@fowler]. The benefits of such an approach with regard to software quality are clear: Errors causing compilation or test failures can hopefully be found immediately after entering the system, and the relevant developer and section of code identified, thus making the debugging swifter and easier[@fowler]. Implementing CI also opens the door to continuous deployment, whereby features are deployed to production individually and almost immediately, rather than waiting for large and infrequent releases[@fowler].

In practice, CI is usually implemented by means of a CI system or tool: There exists a healthy ecosystem of such tools, some designed to be self-hosted on a server within an organisation's firewall (including the open-source CruiseControl[@cc] and Jenkins[@jenkins] applications or Atlassian's commercial, closed-source Bamboo[@bamboo] product) and more modern cloud-based systems such as Travis CI[@travis]. That said, it is perfectly possible to bypass these tools and implement a very simple system based on the event hooks provided by version-control systems. These systems are all centralised to a certain extent, in that there is not a separate instance run on each developer's individual workstation but one per team or per project. Using a single, well-defined environment helps avoid a whole category of issues relating to build configurations, dependency and IDE versions and the like which may differ between individual developers. In the context of integrating mutation testing, it would also allow a single knowledgable developer to configure a mutation testing tool for their whole team.

The usual CI workflow is that, when a change is submitted to the project's version control system (i.e. a commit is pushed to a Git repository, or a changelist submitted to the Perforce mainline) a CI build, that is an integration and testing run, will be immediately triggered[@pracdiff]. A small minority of organisations do prefer to run builds at specific times, perhaps at hourly or daily intervals[@pracdiff]. The build will be uniquely identified by a sequential build number, and then will either pass (if all of the activities run as part of that build report success) or fail if at least one activity reports that the version of the project submitted is in some way unsatisfactory. A failing build will result in the team being notified by means such as email or RSS[@agile], or even lava lamp as per the figure below. The expectation is that "fixing the build", that is making changes so as to trigger a new build that will succeed, will be a high priority task[@fowler]. Regardless of the build outcome, a report is usually produced giving details of the activities performed (usually in terms of the command-line output from the build script), details of the code changes that triggered the build, and further build-related statistics[@agile].

![One way of representing the current CI build status[@llimage]. When the most recent build has passed the green lamp will be illuminated, otherwise the red lamp will be lit and the development team will panic.](images/lava-lamps.png)

A key characteristic of an effective CI workflow seems to be that builds are completed, and their outcome reported, within a reasonably short timeframe. To be of value, builds need to complete quickly enough to give feedback to the relevant programmer while the changes that triggered the build are still at the forefront of their mind[@pracdiff]. [@scaling] reports that, as build times increase, not only does the feedback provided by CI become less useful, developers begin to feel less inclined to frequently push code to the repository feeding the CI system, thus negating some of the benefits that CI brings. [@artagile] gives a rule-of-thumb whereby builds should be kept to less than ten minutes for reasons very similar to those detailed above.

## Combining mutation testing and continuous integration

In an ideal world, having now introduced both mutation testing and continuous integration, the logical next step would be to explore pre-existing papers in the literature that bring the two together. However, as might be expected given the previously-mentioned lack of academic interest in CI, I have been unable to locate any such papers. That said, code coverage scores are somewhat similar to mutation scores, and some interesting work has been done regarding their practical applications. In [@icse], the major pitfall identified is that coverage metrics are quite easy to manipulate (the example given is that the positioning of the braces surrounding blocks can influence the coverage score) and that, combined with the human tendency to optimise work to the criteria by which it will be judged, can lead to rather perverse outcomes. It is important to remember however that, as mentioned above, code coverage is a weaker metric than mutation testing.

More promisingly, one of the main weaknesses inherent in mutation testing, and consequently one of the main problems that a great deal of academic effort has been expended in attempting to address, dovetails quite neatly with one of the aforementioned key characteristics of tasks run with every CI build: This is that mutation testing is a time-consuming process, whereas the utility of a CI build diminishes as the time it takes increases. Indeed, it has been acknowledged that the slowness of mutation testing has been one of the reasons behind its failure to gain widespread industrial adoption[@mt2000].

Intuitively, the reason why mutation testing is time-intensive is immediately obvious: In the worst-case scenario in which all mutants survive (i.e. a score of 0% is returned), a naive mutation tool must execute every test case (which will pass) against each mutant individually. This means that the worst-case time complexity is the product of the number of mutants (denoted as *m*) and the number of test cases (*k*), so *O(mk)*. The received wisdom in much of the literature seems to be that *m* scales with the square of the number of lines of code in the program-under-test (*n*), making the overall time *O(n^2^k)*[@mtcarol]. Certainly, a large number of mutants can be generated for even relatively trivial programs: Coincidentally, both `caseflip` and the equally diminutive demo application given in Appendix 2 produce 19 mutants, while Offutt cites a Fortran program that produces 111 mutants from only 5 statements[@mimd].

[@mt2000] identifies three broad areas into which work to speed up mutation testing can be categorised, namely "do fewer", "do smarter" and "do faster". Work within the first of these seeks to create and score fewer mutants without compromising the usefulness of the results produced too dramatically. Techniques within this category include *selective mutation* (choosing the smallest set of mutation operators that will reveal sufficient gaps in testing) and mutant sampling, which involves randomly deciding which subset of the generated mutants should be scored. "Do smarter" approaches include spreading the scoring workload across multiple machines, as discussed in more depth below, and also so-called *weak mutation* techniques, that seek to decide whether or not a given mutant has been killed based on the program-under-test's internal state after reaching the relevant line, rather than waiting for that program to finish normally. Finally, "do faster" approaches include several novel techniques intended to reduce the time taken for each mutant to compile.

One of the key "do smarter" methods is parallelisation of the mutation scoring process, that is the running of the test suite against each mutant. By definition, the scoring of each individual mutant should run independently of any others, meaning there is little to no synchronisation required between the parallel tasks and so, theoretically, a linear speedup could be possible. Moreover, both mutation tools and testing frameworks are often themselves single-threaded, meaning that, as will be expanded upon later in this report, such an approach wouldn't necessarily require co-operation from the entire toolchain. [@mimd], published in 1992, used what was then quite a novel pseudo-distributed system architecture to explore the performance increases gained by parallelising a Fortran mutation testing tool. As expected, an almost-linear speed increase was found. These results were repeated by a number of other studies[@mtcarol]. Such multiple-core architectures are now commonplace, especially in the workstation- and server-class machines that might be used for CI purposes.

The other major challenge when running mutation testing as part of a CI build is that mutation tools produce a percentage score, while CI simply requires a pass or fail output. The obvious solution to that problem is to treat any mutation score less than 100% as a failure, but this approach is fatally flawed because of the impossibility of automatically and accurately detecting all equivalent mutants: A perfect, and perfectly-tested, codebase could still fall below perfect mutation coverage if an equivalent mutant is produced. While there is work ongoing to try and heuristically identify some equivalent mutants using methods including comparing programs after applying compiler optimisations[@compiler], the problem is, in the general case, undecidable[@regression].

On a more positive note, there is an offhand mention in [@mt2000] of using a mutation threshold (the minimum mutation score required for a successful outcome) of less than 100%, though that paper gives no real indication of what such a threshold might be. Some inspiration can be taken from the world of code coverage, where 85% is apparently a common threshold used by a lot of companies[@icse] but the rationale behind that number is unclear to say the least. The popular Coveralls[@coveralls] cloud-based code coverage system uses three colours to denote coverage levels: Scores below 80% are red, between 80 and 90% are amber and a score of 90% or greater gives a green colour. That service also provides the ability to set a requirement that code coverage should not decrease between builds, which could also be a potential option over in the mutation testing world.

## Project motivation

Finally, a few words of justification for the overall goal of this project: A relevant Offutt quote is that "all respectable software engineering research should have the eventual goal of helping real programmers build better software"[@mtcarol]. This sums up the aim of this project, which is to make a small contribution to enabling the use of mutation testing in the kinds of CI workflows commonly in use in industry and academia.
