\clearpage

# Conclusion

The main contribution of this project has been to provide software capable of running mutation testing against real projects, with acceptable performance, and generating reports suited to a CI environment.

To backtrack a little, the Literature Review, after introducing some basic concepts, highlighted the requirement for processes run as part of a CI build to be reasonably speedy, and gave details of why mutation testing has struggled to meet that need, as well as some potential approaches to remedying that deficiency. There was also a discussion as to how the percentage scores produced by mutation testing could be translated into the binary result required from CI builds.

The Analysis chapter outlined the decisions made in implementing this project's software, in particular the choice to wrap around an existing mutation tool, and to use parallelisation to increase that tool's performance. The Design & Implementation chapter gave more details as to how those decisions were implemented, with a particular focus on the manner in which the application was designed with extensibility in mind. Finally, the Evaluation chapter proved that MultiMutiny can work with real projects, and that, in doing so, it achieves a time-saving of around 60% of the time that, without the benefit of the wrapper this project provides, Mutiny would require.

## Further work

There are several facets of this project that would benefit from further work, particularly in relation to improving the process of scoring parallelisation, to speeding up the system more generally, and to using a better method for setting the mutation threshold in CI mode.

To tackle the first of these, it has been shown in the previous chapter that MultiMutiny's current parallelisation algorithm, while adequate for achieving a reasonable speedup, is not entirely optimal. It may be better to use a work-queue model, whereby, instead of allocating tasks to processes upfront, a queue of mutants is kept, from which idle scoring processes would pick. While this would require more code and greater inter-process communication than there is at present, it does not seem like it would be an enormous change and could help avoid some of the most pathological possibilities inherent in the current system. Splitting the running of a program's test suite between cores could save even more time, but would be much more challenging, if at all possible. Another interesting possibility would be to retain ahead-of-time allocation, but to use data (particularly the time taken in scoring each mutant) from previous runs to inform the allocation. In the case where the program-under-test is more-or-less unchanged, for builds where only variable names or comments have changed for instance, this would almost certainly result in an optimal allocation.

Another possibility with regard to parallelisation would be to split the work across multiple machines. This could be implemented over SSH, with the hypothetical `RemoteScorer` ideally responding to the same interface as the current `LocalScorer`. Distributing the scoring workload in this way would mean that, assuming an infinite availability of machines (which is effectively the case for platforms like Amazon EC2, if one's budget is sufficiently robust) and discounting network overheads, mutation scoring could be performed within the maximum time taken for a single execution of the relevant test suite. This approach would also mean that programs that cannot co-exist with themselves on a single machine (e.g. applications that always bind to a specific port) could still be scored in parallel.

Another possibility for speeding-up the system, at least when running in CI mode, is to halt execution once the outcome of the build is known, i.e. to fail fast. For instance, if the mutation threshold were 60%, scoring could cease when either more than 40% of the tests have failed or 60% have passed. While this approach could result in a marginal speed increase in certain circumstances, it is not as promising as other approaches to this problem: This method has no impact whatsoever on the worst-case runtime, and, assuming that the threshold is set such that most builds are expected to pass, the average speed-up will be marginal. Moreover, users may prefer to be presented with a complete set of mutation results, and an accurate score, rather than a range of possible scores.

The final area for further work identified was in using the mutation score output from the previous test run as the mutation threshold for the following run, thereby mandating a non-decreasing mutation score. This feature was omitted for the reasons given in the Analysis chapter but, if it were decided to increase MultiMutiny's scope to make it the mutation testing equivalent of code coverage products like Coveralls or Codecov, then adding this feature would certainly be a good idea.
